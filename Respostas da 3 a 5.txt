3) Quais cuidados devem ser observados ao capturar dados de um site?
termos de serviço: achar que os termos de serviço não se aplicam a esse caso não é bem verdade. Caso alguém reclame na justiça, as afirmações desses termos podem valer;
leis do local em que o site está hospedado: se o site está hospedado em outro país, o cuidado tem de ser redobrado para não infringir leis locais de proteção aos dados;
taxa de rastreamento: quanto mais rápido os bots trabalham, mais acessos ao servidor. Maior também a chance de o site perceber isso como um ataque. Vá com calma no ritmo de extração;
identificação do Scraper: criar um arquivo de identificação para o seu Scraper, indicando quem você é e como vai usar os dados, é uma boa prática que pode evitar problemas;
proteção dos dados coletados: se os dados que você quer usar têm proteção de direitos autorais, é melhor não coletar. 

4) Quais ameaças capturas automáticas proporcionam para sistemas web?

uma das ameaças e roubar conteúdos protegidos por direitos autorais e gerar cotações de um produto ou serviço para uma concorrência desleal.
5) Você diria que bots ou crawlers são programas facilmente paralelizáveis? Se sim, explique como isso seria implementado dando um exemplo.
Todas as funções de leitura são paralelizadas, de modo que o tempo de leitura dependerá da configuração de sua máquina.




